{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classification - Visual QST & Responsiveness to Treatment Across 5 Chronic Pain Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subid</th>\n",
       "      <th>cohort</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>vis01_unpl_avg</th>\n",
       "      <th>vis02_unpl_avg</th>\n",
       "      <th>vis03_unpl_avg</th>\n",
       "      <th>vis04_unpl_avg</th>\n",
       "      <th>vis05_unpl_avg</th>\n",
       "      <th>vis06_unpl_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>pd02_bsl</th>\n",
       "      <th>wpi_bsl</th>\n",
       "      <th>sss_bsl</th>\n",
       "      <th>fm_score_bsl</th>\n",
       "      <th>pd02_6m</th>\n",
       "      <th>wpi_6m</th>\n",
       "      <th>sss_6m</th>\n",
       "      <th>fm_score_6m</th>\n",
       "      <th>responder_ratio</th>\n",
       "      <th>responder_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>32.333333</td>\n",
       "      <td>36.333333</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>60.666667</td>\n",
       "      <td>61.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>27.333333</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.333333</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subid  cohort  age  sex  vis01_unpl_avg  vis02_unpl_avg  vis03_unpl_avg  \\\n",
       "0      1       0   27    1       32.333333       36.333333       44.000000   \n",
       "1      2       0   30    1             NaN             NaN             NaN   \n",
       "2      3       0   61    1       27.333333       27.000000       39.000000   \n",
       "3      4       0   45    0        0.000000        0.000000        0.000000   \n",
       "4      5       0   53    1       13.000000        8.666667       13.333333   \n",
       "\n",
       "   vis04_unpl_avg  vis05_unpl_avg  vis06_unpl_avg  ...  pd02_bsl  wpi_bsl  \\\n",
       "0       57.000000       60.666667       61.666667  ...       0.0      0.0   \n",
       "1             NaN             NaN             NaN  ...       0.0      0.0   \n",
       "2       38.333333       43.000000       41.666667  ...       0.0      0.0   \n",
       "3        0.000000        0.000000        0.000000  ...       0.0      0.0   \n",
       "4       10.000000        8.000000        7.666667  ...       0.0      0.0   \n",
       "\n",
       "   sss_bsl  fm_score_bsl  pd02_6m  wpi_6m  sss_6m  fm_score_6m  \\\n",
       "0      1.0           1.0      NaN     NaN     NaN          NaN   \n",
       "1      4.0           4.0      NaN     NaN     NaN          NaN   \n",
       "2      0.0           0.0      NaN     NaN     NaN          NaN   \n",
       "3      1.0           1.0      NaN     NaN     NaN          NaN   \n",
       "4      0.0           0.0      NaN     NaN     NaN          NaN   \n",
       "\n",
       "   responder_ratio  responder_bin  \n",
       "0              NaN            NaN  \n",
       "1              NaN            NaN  \n",
       "2              NaN            NaN  \n",
       "3              NaN            NaN  \n",
       "4              NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load in data\n",
    "data = pd.read_csv(\"/Users/noahwaller/Documents/3cohort-GIMME PAPER/csv_for-code/7cohort_visQST_allmetrics_outrem.csv\", delimiter = \",\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subid</th>\n",
       "      <th>cohort</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>vis01_unpl_avg</th>\n",
       "      <th>vis02_unpl_avg</th>\n",
       "      <th>vis03_unpl_avg</th>\n",
       "      <th>vis04_unpl_avg</th>\n",
       "      <th>vis05_unpl_avg</th>\n",
       "      <th>vis06_unpl_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>pd02_bsl</th>\n",
       "      <th>wpi_bsl</th>\n",
       "      <th>sss_bsl</th>\n",
       "      <th>fm_score_bsl</th>\n",
       "      <th>pd02_6m</th>\n",
       "      <th>wpi_6m</th>\n",
       "      <th>sss_6m</th>\n",
       "      <th>fm_score_6m</th>\n",
       "      <th>responder_ratio</th>\n",
       "      <th>responder_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1017</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>37.666667</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>66.333333</td>\n",
       "      <td>71.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1018</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>57.666667</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1007</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>43.666667</td>\n",
       "      <td>48.666667</td>\n",
       "      <td>52.333333</td>\n",
       "      <td>69.333333</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>93.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1012</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1022</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>70.666667</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>61.666667</td>\n",
       "      <td>64.333333</td>\n",
       "      <td>65.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3096</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>78.333333</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>44.333333</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3172</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>75.333333</td>\n",
       "      <td>76.333333</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>71.666667</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3286</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>3007</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>69.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>3269</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>43.333333</td>\n",
       "      <td>49.333333</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subid  cohort  age  sex  vis01_unpl_avg  vis02_unpl_avg  vis03_unpl_avg  \\\n",
       "34    1017       1   45    1       31.000000       37.666667       41.666667   \n",
       "35    1018       1   63    1       11.666667       23.333333       40.000000   \n",
       "36    1007       1   38    1       43.666667       48.666667       52.333333   \n",
       "37    1012       1   62    1        4.666667        2.000000        3.000000   \n",
       "38    1022       1   68    1       70.666667       59.000000       59.000000   \n",
       "..     ...     ...  ...  ...             ...             ...             ...   \n",
       "233   3096       6   40    1       78.333333       63.333333       45.000000   \n",
       "234   3172       6   46    1       75.333333       76.333333       73.333333   \n",
       "235   3286       6   28    1        0.000000        0.000000        0.000000   \n",
       "236   3007       6   54    1       23.000000       33.000000       49.666667   \n",
       "238   3269       6   47    1       56.000000       50.000000       49.666667   \n",
       "\n",
       "     vis04_unpl_avg  vis05_unpl_avg  vis06_unpl_avg  ...  pd02_bsl  wpi_bsl  \\\n",
       "34        56.666667       66.333333       71.666667  ...       9.0      0.0   \n",
       "35        53.333333       57.666667       67.000000  ...       9.0      5.0   \n",
       "36        69.333333       90.000000       93.666667  ...      10.0      4.0   \n",
       "37         3.333333        7.000000       11.000000  ...       9.0      7.0   \n",
       "38        61.666667       64.333333       65.666667  ...       8.0      5.0   \n",
       "..              ...             ...             ...  ...       ...      ...   \n",
       "233       53.000000       44.333333       56.666667  ...      10.0      0.0   \n",
       "234       70.000000       71.666667       73.333333  ...       9.0      8.0   \n",
       "235        0.000000        0.000000        0.000000  ...      10.0     15.0   \n",
       "236       60.000000       70.000000       69.666667  ...       9.0     12.0   \n",
       "238       43.333333       49.333333       46.666667  ...       9.0      5.0   \n",
       "\n",
       "     sss_bsl  fm_score_bsl  pd02_6m  wpi_6m  sss_6m  fm_score_6m  \\\n",
       "34       1.0           1.0      2.0     0.0     1.0          1.0   \n",
       "35       6.0          11.0      2.0     4.0     1.0          5.0   \n",
       "36      10.0          14.0      3.0     0.0     6.0          6.0   \n",
       "37       7.0          14.0      3.0     3.0     8.0         11.0   \n",
       "38       1.0           6.0      7.0     3.0     3.0          6.0   \n",
       "..       ...           ...      ...     ...     ...          ...   \n",
       "233      3.0           3.0      8.0     7.0     6.0         13.0   \n",
       "234     12.0          20.0      8.0     3.0     8.0         11.0   \n",
       "235      2.0          17.0      8.0     5.0     3.0          8.0   \n",
       "236      9.0          21.0      9.0     0.0     9.0          9.0   \n",
       "238      8.0          13.0     10.0     6.0     7.0         13.0   \n",
       "\n",
       "     responder_ratio  responder_bin  \n",
       "34          0.222222            1.0  \n",
       "35          0.222222            1.0  \n",
       "36          0.300000            1.0  \n",
       "37          0.333333            1.0  \n",
       "38          0.875000            0.0  \n",
       "..               ...            ...  \n",
       "233         0.800000            0.0  \n",
       "234         0.888889            0.0  \n",
       "235         0.800000            0.0  \n",
       "236         1.000000            0.0  \n",
       "238         1.111111            0.0  \n",
       "\n",
       "[86 rows x 28 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Requires no missing values\n",
    "data.dropna(subset=['responder_bin'], inplace=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "feature_cols = ['vis01_unpl_avg', 'vis02_unpl_avg', 'vis03_unpl_avg', 'vis04_unpl_avg','vis05_unpl_avg','vis06_unpl_avg']\n",
    "X = data[feature_cols] # Features\n",
    "y = data.responder_bin # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=11, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 5],\n",
       "       [2, 9]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 427.9555555555555, 'Predicted label')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAIQCAYAAACok5ttAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyBklEQVR4nO3deXRT5dr38V8CTSmUqaCUUVSwCCJU5gIyCahLULGggihFscokoIUyCwIFfJDxAQsFlZlHBBkUUBEVFIogHobjwHCQSZnKYEsxpcn7xznkPYEKCew0e9PvZ62sRXb23veV/nHOz+u+c2+b2+12CwAAADfFHuwCAAAAbgWEKgAAAAMQqgAAAAxAqAIAADAAoQoAAMAAhCoAAAADEKoAAAAMQKgCAAAwAKEKAHLAvsgA/EWoAoJs165dSkhIUNOmTXX//ferRYsWGjJkiA4fPhywMT/99FM1a9ZM1atX17Bhwwy7b1RUlKZOnWrY/YJl/fr1GjBgwHXPS0xMVPPmzXOhIgBWYOMxNUDwLFiwQGPGjFG9evX05JNP6vbbb9ehQ4eUkpKiM2fO6L333lO1atUMH7devXqqWLGi+vXrp1KlSqlixYqG3PfHH39UZGSkIiMjDblfsHTu3FmSNG/evGued+jQIaWnp6tq1aq5URYAkyNUAUGyfft2de7cWZ06ddLgwYO9PktLS1O7du1UpEgRrVy50vCxo6Ki1KNHD/Xu3dvwe98KfA1VAPDfmP4DgmT27NkqXLiw+vXrd9VnERERSkxMVKtWrZSenu45/umnn6pdu3aKjo5Ww4YNNWzYMJ07d87z+dSpU9WyZUt99dVXatOmje677z61bt1ay5cvlySlpqYqKipKkvS///u/ioqK0pEjR3Kcxjpy5IiioqK0bNkyz7F58+bp4YcfVvXq1dW4cWO9+eabXvVdOf134sQJDRw4UE2aNNH999+v2NhYrV+/3mucqKgoLViwQIMHD1bdunUVHR2t3r1769SpU3/7t7tc27p169S9e3fVrFlTMTExmj59utLT0zVo0CDVqlVLMTExevvtt73WRx05ckT9+/dXo0aNVK1aNTVo0ED9+/fXmTNnJP07UG3dulVbt25VVFSUUlNTPX+3xYsXq1mzZoqJidGmTZu8/m7r16+/6vv/61//Us2aNX2aSgRgfYQqIAjcbrc2bdqkBg0aKCwsLMdzHn74YfXs2VPh4eGSpOnTp6tv376qUaOGpkyZoh49emjdunXq3LmzLl686Lnu5MmTGjlypJ5//nnNnDlT5cqVU2Jiovbv369q1appyZIlkqTY2FgtWbJEt99+u081f/LJJxo3bpw6deqk2bNnq0ePHlqxYoVGjRqV4/mnTp1SbGystm7dqr59+2rq1KkqW7asevTocVX3beLEiXK5XHrnnXfUv39/ffXVVxozZsx1axo8eLDuuecezZgxQ/Xr19fkyZMVGxurAgUKaPLkyWrevLlSUlK0du1aSVJmZqaef/557d+/X8OHD9fs2bP13HPPafXq1XrnnXckScOHD1fVqlVVtWpVLVmyxGv6deLEiRowYIAGDBigmjVretXSokULPfHEE0pOTtb+/fuVnZ2txMRERUREaOjQoT79jQFYW/5gFwDkRWfOnNFff/2lcuXK+XT+uXPnNGPGDLVv317Dhw/3HL/nnnvUqVMnLVu2TB07dpT07+AwevRoNWjQQJJUsWJFNWvWTF9//bW6du3qCQORkZFXBYNrSU1NVdmyZdWpUyfZ7XbVrVtXBQsW9HR4rvTee+8pLS1Na9asUfny5SVJTZo0UZcuXTR+/Hg99thjstvtnu+RlJTkuXbnzp2eIHQtjRs3Vp8+fSRJlSpV0ieffKISJUp4Ft83bNhQa9as0Q8//KBHHnlEBw8eVGRkpMaOHasKFSpIkurXr69du3Zp69atnvtcDrJX/n2eeeYZPfzww39bz+DBg7V582a9+eabaty4sXbt2qW5c+d67gfg1kanCgiCy2EiOzvbp/N//PFHOZ1OtWnTxut47dq1VbZsWaWmpnod/+8wcHnR+IULF26i4n+Hj4MHD6pdu3aaPn26/vnPf6pNmzZ64YUXcjx/69atio6O9gSqy9q2bauTJ0/qwIEDOdZ7uebMzMzr1hQdHe3592233SZJqlGjhueYzWZT0aJF9eeff0qS7r33Xi1cuFDlypXT4cOHtXHjRs2ZM0cHDhxQVlbWdce7PHX6d4oUKaJRo0Zp69atmjhxorp166batWtf974Abg2EKiAIihUrpkKFCunYsWN/e86FCxd09uxZSfKsmypZsuRV55UsWdITGi777ynFywHuZn+T8uijj2rChAkqWLCgpk2bpieffFItWrTQJ598kuP5586d+9t6Jen8+fM51nu5Zl/qzakD9HfTqZe99957iomJ0UMPPaTExERt2bLlutdcVqJEieue06BBA5UuXVoul4vtFoA8hlAFBEmjRo2Umpqqv/76K8fPly1bpgYNGmjHjh0qWrSoJOW4ePvkyZMqXrz4TdVis9mu6prl1Nl67LHHtHDhQqWmpmrSpEkqVqyYEhISdPz48avOLVq06N/WK+mma74Rq1at0tixY9W1a1dt3rxZ3377rWbOnGnYlhLSv38AcOrUKd19990aMmSInE6nYfcGYG6EKiBIunbtqrNnz2rixIlXfXb69GmlpKTojjvuUM2aNVWjRg05HA6tWrXK67xt27bp2LFjeuCBB26qlkKFCnnWeV32ww8/eJ3Tp08f9ezZU5JUuHBhPfLII+revbuys7N14sSJq+5Zp04d7dix46pNTFeuXKnbbrtNd9xxx03VfCO2b9+uwoUL6+WXX1ZERIQkKSMjQ9u3b5fL5fKcd7m7569du3Zp1qxZeuWVVzRhwgQdOHDgltgMFYBvWKgOBEnNmjX12muvadKkSdq/f7+efPJJFS9eXHv37tWcOXOUkZGhmTNnymazqVixYnr55Zc1bdo0hYSEqEWLFjpy5IgmT56sSpUqqV27djdVS7NmzTRv3jwNGjRI7du399SQL18+zzn169fX8OHDNW7cOD344IM6f/68pk2bpooVK6pKlSpX3TMuLk4rV65UXFycevbsqeLFi+vjjz/Wli1bNGbMmBsOLjfj/vvv16JFizR27Fg1a9ZMJ06c0OzZs3Xq1ClPN1D699qoHTt2aPPmzT5v7Ol0OpWYmKg777xTL7/8shwOh55//nnNnj1bDz30kNdaLwC3JkIVEESvvvqqqlatqgULFigpKUlnz55VZGSkHnzwQb3yyisqU6aM59xevXqpZMmSmj9/vj788EMVK1ZMDz/8sPr06ePzmqC/07BhQw0YMEDz5s3TZ599pmrVqmnatGl65plnPOc888wzysrK0uLFi7Vw4UIVKFBADRo0UEJCgkJCQq6652233aZFixZpwoQJGj16tLKyslSlShVNnz5dLVq0uKl6b9STTz6pI0eO6KOPPtLChQtVqlQpNWnSRB07dtTQoUO1b98+VapUSZ06ddLu3bvVrVs3JSUl+bTtxKRJk3TgwAEtWrRIDodDktS7d2999tlnGjBggFasWKHQ0NBAf0UAQcSO6gAAAAZgTRUAAIABCFUAAAAGIFQBAAAYgFAFAABgAEIVAACAAQhVAAAABiBUAQAAGIBQBQAAYABCFQAAgAEIVQAAAAYgVAEAABiAUAUAAGAAQhUAAIABCFUAAAAGIFQBAAAYgFAFAABgAEIVAACAAQhVAAAABiBUAQAAGIBQBQAAYABCFQAAgAEIVQAAAAYgVAEAABiAUAUAAGAAQhUAAIABCFUAAAAGIFQBAAAYgFAFAABgAEIVAACAAQhVAAAABiBUAQAAGIBQBQAAYABCFQAAgAEIVQAAAAYgVAEIiIMHDwa7BADIVYQqwKKaN2+u6tWrKzo6WtHR0apZs6YaNWqkcePGyeVyGTZO586dNXXqVEnSsGHDNGzYsOte8+WXX+rFF1+84TGXLVum5s2b+/3ZlaZOnarOnTvfcB1RUVFKTU294esB5C35g10AgBs3YsQItWvXzvP+l19+UZcuXRQWFqbevXsbPt7IkSN9Ou/s2bNyu92Gjw8AZkaoAm4hUVFRqlOnjv75z39K+neXqWzZskpNTZXb7dbq1auVlpamMWPGaMeOHSpYsKDatm2rHj16yOFwSJI+/PBDvfvuu0pLS1OrVq2UmZnpuX9iYqIkaezYsZKkDz74QPPnz9epU6d05513KiEhQXa7XcOHD1dWVpaio6O1du1aFS9eXDNmzNDKlSv1559/qkaNGhoyZIjuuOMOSdL+/fv15ptvavfu3SpXrpzq1avn83deunSpFi5cqKNHj8rpdKpu3bpKSkpSRESEJOnChQtKTEzUhg0bFBERofj4eD3xxBOSJKfTec26AMAfTP8Bt4isrCylpqZqy5Ytatiwoef4d999p8WLF2vlypWy2+3q0qWLKleurG+++UYLFy7Ud99955ne27x5s0aOHKlRo0bp+++/V40aNbRr164cx1u2bJmmT5+u8ePHa/v27Xr22Wf16quvKioqSiNGjFCZMmW0Y8cOlSpVShMnTtRXX32l999/Xxs3blSNGjXUtWtX/fXXX8rKylJ8fLwqV66sLVu26J133tEXX3zh03feuXOnRo0apTfffFOpqalas2aNDh48qLlz53rO2b17t+677z5t2rRJQ4YM0ZAhQ7Rt2zZJumZdAOAvQhVgYSNGjFDt2rVVu3ZtNWjQQG+99Zbi4uL03HPPec558MEHVapUKRUpUkRfffWVnE6n+vXrp9DQUJUuXVqvvfaaFixYIElauXKlWrVqpQYNGih//vzq2LGjqlatmuPYy5cv19NPP63o6GjZ7Xa1b99ec+bMUYECBbzOc7vdWrx4sfr166fy5csrNDRUPXr0UFZWlr766ivt2LFDv//+u/r376/Q0FBVrlxZcXFxPn3/e+65R6tXr9b999+vc+fO6cSJE4qIiNDx48c959x777167rnnFBISooYNG6p169ZasWLFdesCAH8x/QdY2PDhw73WVOXk9ttv9/z76NGjSktLU506dTzH3G63srKydPr0aR0/flzVqlXzur58+fI53vfkyZMqU6aM17EHHnjgqvPS0tJ04cIFvfbaa7Lb//9/x2VlZXmm7IoXL+4VxipUqHDN73SZ3W7X3LlztWrVKhUsWFBRUVFKT0/3Ws9Vrlw5r2tKly6tX3/99bp1AYC/CFXALc5ms3n+HRkZqQoVKmjt2rWeY+np6Tp9+rQiIiIUGRmpw4cPe13/xx9/qHLlylfdt3Tp0vr999+9jk2cOFFt27b1Ola8eHGFhoZqzpw5qlmzpuf4gQMHVKpUKf30009KS0tTRkaGChUq5BnTF++//76+/fZbrVq1SiVLlpQkvfLKK17nnDhxwuv94cOHVbZs2evWBQD+YvoPyEOaNWumjIwMpaSkyOl06vz58xowYID69u0rm82mp556Sl988YU2bNigS5cuafny5frHP/6R473atWunJUuWaOfOnXK5XProo4+0YMECT1jJzMzUpUuXZLfbFRsbqwkTJuiPP/6Qy+XS8uXL9dhjj+m3335TdHS07rzzTo0aNUqZmZn67bffNGfOHJ++T3p6uvLnz6+QkBBdunRJK1as0MaNG5WVleU5Z+fOnfroo4+UlZWlDRs26Msvv1T79u2vWxcA+ItOFZCHhIeH6/3339fYsWOVkpIil8ulevXqacaMGZKkWrVqafz48Ro7dqz69u2r+vXrey16/29t2rTR+fPnlZCQoJMnT6pSpUqaNWuWIiIiVKdOHZUoUUJ16tTR4sWLNWDAAE2dOlUdO3bU2bNnVb58eU2ZMsWzXmvmzJkaNmyYYmJiVLJkSbVo0UKfffbZdb9P165d9euvv6pZs2YKDQ1V1apV1bFjR23ZssVzTkxMjNavX69Ro0apXLlymjx5smfc69UFAP6wudlMBgAA4KYx/QcAAPAf+/fv14svvqjatWuradOmmjFjhs9PqSBUAQAASMrIyNBLL72k0qVL65tvvtGCBQv06aefavr06T5dT6gCAACQtH37dp0+fVrDhg1TwYIFVbZsWb366qtatGiRT4/eIlQBAABIcrlcCgkJUUhIiOeYzWbTqVOndP78+etef4v8+u/XYBcAwEcvbzoW7BIA+GBmo6bBLsEwTqdTTqfT65jD4fA88/SyBx54QAUKFNCECRPUo0cPpaWlafbs2ZKkixcvqmjRotcc5xYJVQAAwOrCKjwbkPuOT4jRtGnTvI717NlTvXr18jpWpEgRzZo1S0lJSWratKkqVKigJ554Qrt27VKRIkWuOw6hCgAAmILNFphVSfHx8Vc9U/TKLpX0747WpUuXNHfuXM/TKBYuXKhKlSopLCzsuuOwpgoAANzSHA6HwsPDvV45hSpJevHFF7V06VK53W7t3r1b7777rl544QWfxqFTBQAATMEW5F6Pw+HQ9OnTlZSUpDFjxqhEiRLq1q2bOnTo4NP1hCoAAGAKgZr+80edOnW0bNmyG7o2+NUDAADcAuhUAQAAUzBDp+pmWLt6AAAAk6BTBQAATOHyNgZWRagCAAAmYe0JNGtXDwAAYBJ0qgAAgCmwUB0AAAB0qgAAgDlYvVNFqAIAAKYQ7MfU3CxrVw8AAGASdKoAAIApWH36z9rVAwAAmASdKgAAYApW71QRqgAAgClYPVRZu3oAAACToFMFAABMwSZrP1CZThUAAIAB6FQBAABTsPqaKkIVAAAwBauHKmtXDwAAYBJ0qgAAgCnQqQIAAACdKgAAYBbW7vUQqgAAgCkw/QcAAAA6VQAAwBzoVAEAAIBOFQAAMAebxXs9hCoAAGAKTP8BAACAThUAADAHm80W7BJuCp0qAAAAA9CpAgAApmD1NVWEKgAAYApW//WftasHAAAwCTpVAADAFKw+/Wft6gEAAEyCThUAADAFq3eqCFUAAMAUWKgOAAAAOlUAAMAkLD79Z+3qAQAATIJOFQAAMAUWqgMAABiAByoDAACAThUAADAHq2+pQKgCAACmYPU1VdauHgAAwCToVAEAAHNgoToAAAAIVQAAwBzsAXr5Yc+ePerUqZNq166tRo0aadSoUXI6nT6XDwAAEHw2W2BePnK5XIqPj1fr1q21detWLV26VJs2bdKsWbN8up5QBQAAIOncuXM6efKkXC6X3G63JMlutyssLMyn61moDgAAzCFAC9WdTudVU3gOh0MOh8PrWPHixdWlSxeNGzdO48ePV3Z2tlq0aKEuXbr4NA6dKgAAcEtLTk5WrVq1vF7JyclXnedyuVSgQAENHTpUP/74o1avXq39+/drypQpPo1DpwoAAJhDgFo98fHxiouL8zp2ZZdKkj7//HOtW7dOa9eulSRVrlxZPXr00OjRo9WnT5/rjkOoAgAApuAO0PRfTlN9Ofn999+vmibMnz+/QkJCfBqH6T8AAABJjRo10smTJ/Xuu+8qOztbhw8f1owZM9SmTRufridUAQAAc7AF6OWjSpUqKTk5WV9++aXq1aun559/Xs2bN1ffvn19up7pPwAAgP+IiYlRTEzMDV1LqAIAAOZgt/az/whVAADAHHigMgAAAOhUAQAAc7B2o4pOFQAAgBHoVAEAAHNgoToAAIABWKgOAAAAOlUAAMAcrN2oolMFAABgBDpVAADAHFioDgAAYABrZyqm/wAAAIxApwoAAJiCmy0VAAAAQKcKAACYAwvVAQAADGDtTMX0HwAAgBHoVAEAAHNgoToAAADoVAEAAHNgoToAAIABrJ2pmP4DAAAwAp0qAABgDixUBwAAAJ0qAABgDhbvVBGqAACAOVh8/szi5QMAAJgDnSoAAGAOFp/+o1MFAABgADpVAADAHKzdqCJUAQAAc3Bb/DE1TP8BAAAYgE4VAAAwB4svVCdUwVR+/vlfGjdujvbs2aeQkPxq2DBaiYkvKiKiaLBLA3CFE1u/10+z5sgeEuI5VjK6pu7t1jWIVcHSrJ2pCFUwj4sX/9JLL72pDh1aKTl5mDIyMjVgwEQNGjRZ7747LNjlAbjCn//6TaUa1FOVrl2CXQpgCqypgmkcO3ZSVapUVI8ez8jhCFHx4kX09NMP6/vv9wS7NAA5+PPgQRWueEewy8CtxG4LzCuXBK1TlZ6eroyMDBUqVEjh4eHBKgMmctdd5ZSSMsLr2Lp136patUpBqgjA33G7XEr/7ZDyhYbq8JrP5Ha7VKL6fboztp1CChUKdnlAUORqqHK5XHr//fc1f/58/f77757jkZGRio2NVffu3WWz+CI1GMPtdmvSpPnasGGr5s8fG+xyAFwh6890hVcor5K1HlDVV+OVlZ6un2e/p59nzVH1Pr2CXR6syuIZIFdD1dixY7V582a98cYbqlSpksLCwpSZmal9+/ZpxowZunDhghISEnKzJJhQevoFDRw4SXv27Nf8+WMVFVUx2CUBuIKjaBHVTPz//3udLzRCd7V/SjtGj9WlzIvKH1YgiNXBsqydqXI3VK1atUoffvihypUr53X8nnvuUfXq1fXMM88QqvK4Q4d+V7duI1SmzG1auvQdfvUHmFT64SM6kbpVdz71pGeGwX3pkmSzyZ4/X5CrA4IjV0PVpUuXdPvtt+f4WUREhLKzs3OzHJjMuXPpeuGFwapf/36NHt1bdju/owDMKqRQIR398ivlL1RI5Vs9pL/OntP+/1uqyIYNvLZYAPxi8R3VczVU1a1bV0OGDFH//v1VsmRJz/G0tDSNHj1a9erVy81yYDLLln2hY8dOas2aTVq79luvz3bs+DBIVQHISWhEcVV/raf+9dFyHVr9qewhIbq9bm3d1f6pYJcGBI3N7Xa7c2uwtLQ0vfbaa9q2bZuKFi2qggULKjMzU2fPnlWtWrU0ZcoURURE3MCdfzW8VgCB8fKmY8EuAYAPZjZqmutj3v1iYP4Dev/s9gG575VytVMVERGhefPm6dChQ9q7d68yMjJUsGBBVa5cWXfcwV4nAADkZW5rz/4FZ5+qChUqqEKFCsEYGgAAICB4TA0AADAHiy9U5+dVAAAABqBTBQAAzIEd1QEAAAxg8ek/QhUAAICklStXavjw4V7HsrKyJEm7d+++7vWEKgAAYA5BXundtm1btW3b1vP++PHjeuqpp3x+hB4L1QEAAK7gdruVkJCgpk2b6vHHH/fpGjpVAADAHEy0UH3FihXat2+fpk+f7vM1hCoAAGAOAVqo7nQ65XQ6vY45HA45HI4cz3e5XJoxY4ZeeeUVhYeH+zwO038AAOCWlpycrFq1anm9kpOT//b81NRUnThxQrGxsX6NQ6cKAACYgjtA03/x8fGKi4vzOvZ3XSpJWrdunVq2bKmCBQv6NQ6hCgAA3NKuNdWXk+3bt+v555/3exxCFQAAMAeTLEo6cuSIbr/9dr+vI1QBAABzMMmO6jt27Lih60ySCQEAAKyNThUAADAHE+1TdSPoVAEAABiAThUAADAHk6ypulGEKgAAYA7WzlRM/wEAABiBThUAADAFt8Wn/+hUAQAAGIBOFQAAMAeLd6oIVQAAwBzYpwoAAAB0qgAAgDlYvNVj8fIBAADMgU4VAAAwB4uvqSJUAQAAc7D4r/+Y/gMAADAAnSoAAGAOdKoAAABApwoAAJiCm4XqAAAABrD4/JnFywcAADAHOlUAAMAcmP4DAAAwAL/+AwAAAJ0qAABgDnSqAAAAQKcKAACYg7UbVYQqAABgDm6m/wAAAECnCgAAmIPF96miUwUAAGAAOlUAAMAcLL6milAFAADMwdqZiuk/AAAAI9CpAgAApmC3eKvH4uUDAACYA50qAABgChbfUYFQBQAAzMHqoYrpPwAAAAPQqQIAAKZgs3irik4VAACAAehUAQAAU7B4o4pQBQAAzMHqoYrpPwAAAAPQqQIAAKZgs3irx+LlAwAAmAOdKgAAYApWX1NFqAIAAKZgt3ioYvoPAADAAHSqAACAKVh9+o9OFQAAgAEIVQAAwBRstsC8/HH27Fn1799f9erVU506ddS9e3edOHHCp2t9mv6rUqXKdR9y+NNPP/k0IAAAQE7M8EDlXr16qWjRovr8889lt9s1cOBADR06VMnJyde91qdQNXfu3JsuEgAAwMx2796tf/zjH/ruu+8UHh4uSXrrrbd08uRJn673KVTVrVvX6/25c+d0+PBhVa1aVZcuXZLD4fCzbAAAAG+B2lHd6XTK6XR6HXM4HFfll507d6pSpUr6v//7Py1atEiZmZlq3LixBgwY4NM4fpWfkZGh119/XfXq1dNzzz2ngwcPqmXLljpw4IA/twEAAMg1ycnJqlWrltcrp+m8c+fO6ZdfftHBgwe1fPlyffzxxzp+/LjPocqvLRXGjx+vCxcuaM2aNerQoYPKly+vZs2aafTo0Zo9e7Y/twIAAPASqCVV8fHxiouL8zqW0yzb5WODBw9WaGiowsPD1adPH3Xo0EEZGRkqVKjQNcfxK1Rt2LBBq1atUtGiRWWz2RQSEqLExEQ9+OCD/twGAADgKoEKVTlN9eWkUqVKcrlcysrKUmhoqCTJ5XJJktxu93Wv92v6z+VyeYq6fPP/PgYAAGBVMTExKl++vAYNGqSMjAylpaVp4sSJeuihhzwL16/Fr1BVv359jRw5UpmZmZ6fPU6aNOmqhewAAAD+CvY+VSEhIZo3b57y5cun1q1bq3Xr1oqMjNSYMWN8ut6v6b+BAwfq1VdfVZ06dZSdna3o6GhVrFhR7777rj+3AQAAMKVSpUpp4sSJN3StX6GqRIkSWrJkiXbt2qWjR48qMjJS999/v/Lly3dDgwMAAFxmD/7enzfF7wcqZ2Rk6PDhwzp+/LjsdruysrIIVQAA4KaZYEP1m+JXqNq1a5deeuklFShQQJGRkTp69KjGjRunlJQU3XXXXYGqEQAAwPT8WqielJSkuLg4ff3111qyZIk2btyoxx9/XCNHjgxUfQAAII8I9kL1m+VXqNq3b5+6devmeW+z2dS9e3ft3r3b8MIAAACsxK9QFRUVpR9//NHr2E8//aTy5csbWRMAAMiDbHZbQF65xac1VdOmTZMklS5dWvHx8YqNjVW5cuV04sQJLV26VK1atQpokQAA4NaXJxaqp6amev597733as+ePdqzZ48k6e677+aBygAAIM/zKVTNmzcv0HUAAIA8Lk90qv7bli1bdPz4cc+z/7KysvTLL79oyJAhhhcHAADyjjwVqkaNGqXFixerUKFCkqTs7GxlZGSocePGASkOAADAKvwKVWvWrNH8+fOVmZmplStXasyYMRo3bpwuXLgQqPoAAEAekaceU5OZmamaNWvq5MmT2rNnj2w2m3r27KlHH300UPUBAABYgl+hKjIyUqdPn9Ztt92mP/74Q1lZWSpQoIDS09MDVR8AAMgj8tSaqiZNmqhLly764IMPVKdOHQ0aNEihoaGqWLFigMoDAAB5hc2vLcnNx6/y+/Xrp8cff1whISEaNmyYzpw5o3379umtt94KVH0AAACW4FenKiQkRC+99JIkqXDhwkpJSQlIUQAAIO/JE9N/AwcOvO45SUlJN10MAACAVfm9+ScAAEAg2CzeqvIpVNGFAgAAgWbxTOXfQnUAAADkjOk/AABgCnSqAAAAQKcKAACYg9U7VWypACBXzeuYHOwSAPhg5qGmuT6m1R+ozPQfAACAAdhSAQAAmILVO1V+ralyOp1atWqVjh8/LpfLJUnKysrSr7/+qhkzZgSkQAAAACvwK1QNGjRIGzduVPHixZWVlaWCBQtq7969euKJJwJUHgAAyCvsNnewS7gpfoWqjRs3atGiRUpLS9OiRYs0YcIEzZkzRzt37gxUfQAAII+w+vSfXwvVXS6X7rrrLt1111366aefJEmdOnXStm3bAlIcAACAVfjVqYqMjNThw4dVvnx5nT59WhcuXJDdbldGRkag6gMAAHmE1bck8CtUtWnTRh07dtTSpUvVtGlTvfrqqwoNDdV9990XqPoAAAAswa9Q9fLLL6t8+fIqXLiwhg4dqrffflvp6ekaOnRooOoDAAB5RJ5aqC5JjzzyiOffI0aMMLQYAACQd1l9obpfoapz586y/c2DeebOnWtIQQAAAFbkV6iqV6+e1/szZ85o7dq1evrppw0tCgAA5D15aqF6z549rzrWrl07jR8/3rCCAAAArMjvNVVXqlatmnbv3m1ELQAAIA/LU2uqjh075vU+KytLn3zyiUqXLm1oUQAAIO+x5aVf/zVv3txrobrb7VbRokX11ltvGV4YAACAlfgVqtavX+/1Pl++fCpRooRCQkIMLQoAAOQ9Vp/+82uh/ahRo1S2bFnPKzIyUiEhIXruuecCVR8AAIAlXLdTdeTIEX388ceSpE2bNmnatGlen6enp+uXX34JSHEAACDvuOW3VChTpoz27t2rtLQ0ZWdnKzU11evz0NBQDR8+PGAFAgCAvOGWf0yN3W7X5MmTJUlDhgzRqFGjAl4UAACA1fjVaevfv79ef/117d+/X5I0efJkJSQkKCMjIyDFAQCAvMNuC8wr1+r35+QRI0bo3LlzKlasmCTpscce059//qkxY8YEojYAAADL8GtLhW+//Vbr169XoUKFJEl33323/ud//kctW7YMSHEAACDvuOUXqv83l8ul7Oxsr2Nut1v58uUztCgAAJD35Kl9qh588EENGDBAhw4dUlZWlg4dOqSBAweqYcOGgaoPAAAg13z66aeqWrWqoqOjPa+EhASfrvWrUzVo0CC99tpratWqledxNTExMRo5cqT/VQMAAPwXM2ypsGvXLj3++ONKSkry+1q/QlVERITmzZunY8eO6eTJk8rOztbHH3+s5s2b68cff/R7cAAAADPZtWuXHnnkkRu61q9QddmxY8c0e/Zsff3116pcubLPbTEAAIC/E+w1VS6XS3v27FFYWJhSUlKUnZ2tJk2a6I033lDRokWve73Pocrlcmnt2rV67733tHfvXl26dEnJyclq3LjxTX0BAAAAKXC//nM6nXI6nV7HHA6HHA6H17G0tDRVrVpVrVu31pQpU3TmzBkNGDBACQkJmjlz5nXH8SlUffDBB5o7d65cLpeeffZZzZo1Sw8//LDuueceP74SAABA7ktOTr7q2cU9e/ZUr169vI6VLFlSCxYs8LwPCwtTQkKCOnTooPT0dIWHh19zHJ9CVVJSkjp27KjExMSrUh0AAIARArVQPT4+XnFxcV7HcsozP//8s1avXq3XX3/d84M8p9Mpu93uU/7xqdM2dOhQpaamqkmTJpo4caKOHz/uGQwAAMAIgXpMjcPhUHh4uNcrp5BUrFgxLViwQCkpKbp06ZKOHTumt99+W08++aRxoapTp0765JNP9M4772jfvn1q2bKlzp8/r82bN1+1GSgAAIAVRUZGKjk5WevXr1fdunX11FNPqXr16ho2bJhP19vcbrffvbajR49q4cKF+uijj2S329W2bVslJib6Xbxxfg3i2AD8EVZheLBLAOCDzEOLcn3Mnps3BOS+0xo0C8h9r3RDC+3Lli2rhIQEffPNN+rXr5+2bt1qdF0AAACWckP7VF3mcDgUGxur2NhYo+oBAAB5VJ56oDIAAECgmOExNTfD6qEQAADAFOhUAQAAUwj2Y2puFp0qAAAAA9CpAgAApmD1Tg+hCgAAmALTfwAAAKBTBQAAzMHGlgoAAACgUwUAAEzB6muqCFUAAMAUrD59ZvX6AQAATIFOFQAAMAWe/QcAAAA6VQAAwBxYqA4AAGAAq4cqpv8AAAAMQKcKAACYQr5gF3CT6FQBAAAYgE4VAAAwBatvqUCoAgAApsBCdQAAANCpAgAA5kCnCgAAAHSqAACAOeSzeKeKUAUAAEyB6T8AAADQqQIAAOZg9X2q6FQBAAAYgE4VAAAwBauvqSJUAQAAU+CBygAAAKBTBQAAzMHq0390qgAAAAxApwoAAJiC1bdUIFQBAABTsPpjapj+AwAAMACdKgAAYAosVAcAAACdKgAAYA5W71QRqgAAgClYPVQx/QcAAGAAOlUAAMAU8ll8nyo6VQAAAAagUwUAAEzB6p0eQhUAADAFFqoDAACAThUAADAHq3eqCFUAAMAU+PUfAADALSQ7O1udO3dWYmKiX9cRqgAAgCnYbYF5+WvatGnatm2b//X7PxQAAMCtafPmzfrss8/UqlUrv68lVAEAAFMIVKfK6XQqPT3d6+V0Oq8a//Tp0xo8eLAmTJigsLAwv+tnoToAADCFQP36Lzk5WdOmTfM61rNnT/Xq1cvz3uVyKSEhQXFxcapSpcoNjUOoAgAAt7T4+HjFxcV5HXM4HF7vk5OT5XA41Llz5xseh1AFAABMIV+AOlUOh+OqEHWlFStW6MSJE6pdu7Yk6eLFi5KkL774wudF64QqAACQ561du9br/eXtFMaOHevzPQhVAADAFOwW3/yTUAUAAEzBTFsS+NOhusxM9QMAAFgWnSoAAGAKVn+gMp0qAAAAA9CpAgAAphCoLRVyC6EKAACYgtV//cf0HwAAgAHoVAEAAFNgoToAAADoVAEAAHOweqcq10PV999/f91z6tSpkwuVAAAAM7H69Fmuh6rBgwfr8OHDcrtzXuFvs9n0008/5XJVAAAANyfXQ9XixYv1zDPPqG/fvnrkkUdye3gAAGBSNotP/+V6py0iIkJJSUl6++235XK5cnt4AACAgAjK9GWtWrXUu3dvnTlzJhjDAwAAE7IF6JVbgvbrvyeeeCJYQwMAABNi+g8AAADsUwUAAMzB6p0eq9cPAABgCnSqAACAKdhsOe9haRWEKgAAYAoWX6fO9B8AAIAR6FQBAABTYEsFAAAA0KkCAADmYPFGFaEKAACYg93iqYrpPwAAAAPQqQIAAKZg8UYVnSoAAAAj0KkCAACmYPUtFQhVAADAFCyeqZj+AwAAMAKdKgAAYAp0qgAAAECnCgAAmIPVN/8kVAEAAFOweKZi+g8AAMAIdKoAAIAp2GzuYJdwUwhVAADAFJj+AwAAAJ0qAABgDlZ/TA2dKgAAAAPQqQIAAKZg9U4PoQoAAJgC038AAACgUwUAAMzB4o0qOlUAAABGoFMFAABMweprqghVAADAFCyeqZj+AwAAMAKdKgAAYAp2i7eq6FQBAAD8x+bNm9W+fXs98MADatiwod566y1dvHjRp2sJVQAAwBRsAXr5Ki0tTfHx8Xr22We1bds2LV++XFu3btXMmTN9up7pPwAAYAo2mzuo40dEROi7775TeHi43G63zp49q7/++ksRERE+XU+oAgAA+I/w8HBJUpMmTXT8+HHVrl1b7dq18+lapv8AAIApBGr6z+l0Kj093evldDqvWctnn32mb775Rna7Xb179/apfkIVAAC4pSUnJ6tWrVper+Tk5GteU6BAAZUqVUoJCQnauHGjzp07d91xmP4DAACmEKgd1ePj4xUXF+d1zOFwXHXeDz/8oEGDBmnlypWez51Op0JCQhQWFnbdcehUAQAAUwjU9J/D4VB4eLjXK6dQFRUVpYsXL2rChAlyOp06evSoxo0bp9jY2BzPvxKhCgAAQFKhQoWUkpKivXv3qmHDhurcubNiYmI0aNAgn65n+g8AAJiCGTo9lSpV0pw5c27oWjPUDwAAYHl0qgAAgCkEaqF6biFUAQAAk7B2qmL6DwAAwAB0qgAAgCnY6FQBAACAThUAADAFm83avR5CFQAAMAmm/wAAAPI8OlUAAMAUWKgOAAAAOlUAAMAsrN2pIlQBAABTsPqv/6xdPQAAgEnQqQIAACZh7ek/OlUAAAAGoFMFAABMwepbKhCqAACAKVg9VDH9BwAAYAA6VQAAwCSs3euxdvUAAAAmQacKAACYgs1m7TVVhCoAAGAS1g5VTP8BAAAYgE4VAAAwBatvqUCoAgAAJmHtCTRrVw8AAGASdKoAAIApWH36j04VAACAAehUAQAAU2CfKgAAAENYO1Qx/QcAAGAAOlUAAMAUbBbv9Vi7egAAAJOgUwUAAEzC2muqCFUAAMAUrP7rP6b/AAAADECnCgAAmASdKgAAgDyPThUAADAFq2+pQKgCAAAmwfQfAABAnkenCgAAmIKNThUAAADoVAEAAFOw+uafhCoAAGAS1p5As3b1AAAAJkGnCgAAmAIL1QEAAECnCgAAmIW1O1WEKgAAYApW//Uf038AAAD/8fPPPysuLk5169ZVw4YN1b9/f6Wlpfl0LaEKAACYhD1AL99cvHhRL730kqKjo7Vp0yatXr1aZ8+e1aBBg3yuHgAAIM87duyYqlSpoh49esjhcKh48eJ6+umn9f333/t0PWuqAACAKQRqSwWn0ymn0+l1zOFwyOFweB276667lJKS4nVs3bp1qlatmk/j3CKh6p5gFwDAR5mHFgW7BACmFZj/P09Onqpp06Z5HevZs6d69er1t9e43W5NmjRJGzZs0Pz5830ax+Z2u903VSkAAICJ+dqpuiw9PV0DBw7Unj17NGPGDEVFRfk0zi3SqQIAAMjZtQLUlQ4dOqRu3bqpTJkyWrp0qSIiInweh4XqAAAAks6dO6cXXnhBDzzwgGbPnu1XoJKY/gMAAJAkvffeexo7dqzCwsKu2oh0x44d172eUAUAAGAApv8AAAAMQKgCAAAwAKEKAADAAIQqAAAAAxCqYDqnT59W9+7dVbt2bdWrV0+jR4/WpUuXgl0WgL+Rlpamli1bKjU1NdilAEFFqILp9OnTRwULFtTGjRu1dOlSbd68We+//36wywKQg+3bt+vpp5/WoUOHgl0KEHSEKpjKb7/9pq1btyohIUFhYWEqX768unfvrgULFgS7NABXWL58ud544w317ds32KUApkCogqns3btXxYoVU6lSpTzH7r77bh07dkznz58PYmUArtSoUSN9/vnnevTRR4NdCmAKhCqYSkZGhsLCwryOXX5/4cKFYJQE4G/cdtttyp+fR8gClxGqYCoFCxZUZmam17HL7wsVKhSMkgAA8AmhCqZSuXJlnT17VqdOnfIc279/vyIjI1W4cOEgVgYAwLURqmAqFStWVK1atTRmzBilp6fr8OHDmj59umJjY4NdGgAA10SogulMmTJFly5dUosWLdShQwc1btxY3bt3D3ZZAABck83tdruDXQQAAIDV0akCAAAwAKEKAADAAIQqAAAAAxCqAAAADECoAgAAMAChCgAAwACEKgAAAAMQqgAAAAxAqAIAADAAoQoAAMAAhCoAAAADEKoAAAAM8P8AuX+9zy/xaN4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahwaller/opt/anaconda3/envs/erdos_sp_2024/lib/python3.11/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1 features, but LogisticRegression is expecting 6 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# With classifications we have a new method\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# predict_proba which returns the probability\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# that an observation is a certain class.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1000\u001b[39m),\n\u001b[0;32m----> 8\u001b[0m             \u001b[43mlogreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[:,\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr--\u001b[39m\u001b[38;5;124m'\u001b[39m,linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.5\u001b[39m,label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Fit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(X_train,y_train,label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Data\u001b[39m\u001b[38;5;124m'\u001b[39m,alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.7\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend(fontsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m,loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erdos_sp_2024/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1384\u001b[0m, in \u001b[0;36mLogisticRegression.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1376\u001b[0m ovr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     )\n\u001b[1;32m   1382\u001b[0m )\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ovr:\n\u001b[0;32m-> 1384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_proba_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1386\u001b[0m     decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erdos_sp_2024/lib/python3.11/site-packages/sklearn/linear_model/_base.py:466\u001b[0m, in \u001b[0;36mLinearClassifierMixin._predict_proba_lr\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict_proba_lr\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    460\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Probability estimation for OvR logistic regression.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    Positive class probabilities are computed as\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m    1. / (1. + np.exp(-self.decision_function(X)));\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    multiclass is handled by normalizing that over all classes.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     expit(prob, out\u001b[38;5;241m=\u001b[39mprob)\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prob\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erdos_sp_2024/lib/python3.11/site-packages/sklearn/linear_model/_base.py:432\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    429\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    430\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 432\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erdos_sp_2024/lib/python3.11/site-packages/sklearn/base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erdos_sp_2024/lib/python3.11/site-packages/sklearn/base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 1 features, but LogisticRegression is expecting 6 features as input."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot figure \n",
    "plt.figure(figsize = (7,5))\n",
    "\n",
    "# With classifications we have a new method\n",
    "# predict_proba which returns the probability\n",
    "# that an observation is a certain class.\n",
    "plt.plot(np.linspace(0,1,1000),\n",
    "            logreg.predict_proba(np.linspace(0,1,1000).reshape(-1,1))[:,1],\n",
    "            'r--',linewidth=2.5,label = \"Model Fit\")\n",
    "plt.scatter(X_train,y_train,label = 'Training Data',alpha=.7)\n",
    "plt.legend(fontsize = 14,loc = 4)\n",
    "plt.xlabel(\"Feature\",fontsize = 12)\n",
    "plt.ylabel(\"p(X)\",fontsize=12) \n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_sp_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
